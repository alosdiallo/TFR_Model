---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---

Here is my toolbox for the project.
```{r message=FALSE, include=FALSE}
library(neuralnet)
library(readxl)
library(grid)
library(corrplot)
library(caret)
library(e1071)
library(ROCR)
library(ggplot2)
library(GGally)
library(PerformanceAnalytics)
library(factoextra)
library(corrplot)
library(Rtsne)
library(FactoMineR)
library(ggplot2)
library(factoextra)
library(survminer)
library(ggcorrplot)
library(readr)
library(circlize)
library(readxl)
library(stringr)
library(reshape)
library(psych)
library(ComplexHeatmap)
library(ggpubr)
library(readr)
library(gridExtra)
library(cowplot)
library(MASS)
library(fitdistrplus)
#BiocManager::install("preprocessCore")
library(preprocessCore)
library(MASS)
library(class)
library(ggplot2)
library(reshape2)
library(ROCR)
library(e1071)
library(GGally)
library(klaR)
library(randomForest)
```

```{r}
model_4_21_95wRAW = NULL
model_2_7_21_new = NULL
model_2_7_21_new <- read_excel("F:/Research/Sage/2021 Model/model_2_7_21_newV2.xlsx")

#model_2_7_21_new2 = apply(model_2_7_21_new[,2:104],2,as.numeric)
#model_4_21_95wRAW <- cbind(model_2_7_21_new[,1],model_2_7_21_new2)

#View(model_4_21_95wRAW)  
model_4_21_95wRAW <- model_2_7_21_new

library(tidyverse)
model_4_21_95wRAW %>% select_if(negate(is.numeric))
```


```{r}
summPreds <- function(inpPred,inpTruth,inpMetrNms=c("err","acc","sens","spec")) {
  retVals <- numeric()
  for ( metrTmp in inpMetrNms ) {
    retVals[metrTmp] <- performance(prediction(inpPred,inpTruth),measure=metrTmp)@y.values[[1]][2]
  }
  retVals
}
```





```{r}
dim(model_4_21_95wRAW)
multi.hist(model_4_21_95wRAW[,2:103]) 
#pairs(model_4_21_95wRAW[,2:44])
```


PCA
```{r}



#here we are computing PCA

prcompTmp <- prcomp(model_4_21_95wRAW[,2:103],center = TRUE,scale = TRUE)

#Here we have a plot of the variance by principle componant
plot(prcompTmp)

#This is just a fancy way of doing the same thing
fviz_eig(prcompTmp, addlabels=TRUE, ylim=c(0,60), geom = c("bar", "line"), barfill = "gold", barcolor="grey",linecolor = "red", ncp=10)+
labs(title = "Principal Component Analysis Immune Cell Data",
         x = "Principal Components", y = "% of variances")


all_var <- get_pca_var(prcompTmp)
all_var

#Correlation between variables and PCA
#It shows the importance of a principal component for a given observation (vector of original variables). You can go through the following link for details.
corrplot(all_var$cos2, is.corr=FALSE)

#To highlight the most contributing variables for each components
corrplot(all_var$contrib, is.corr=FALSE)    
# 
pdf("cor_plots.pdf")
corrplot(all_var$contrib, is.corr=FALSE) 
dev.off()

fviz_pca_var(prcompTmp, col.var = "black")


fviz_pca_var(prcompTmp, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )




```

Test area for models
```{r}
    Train = NULL
    Test = NULL
    bTrain = sample(1:nrow(model_4_21_95wRAW),round(0.7*nrow(model_4_21_95wRAW)))
    Train <- model_4_21_95wRAW[bTrain,2:104]
    Test <- model_4_21_95wRAW[-bTrain,2:104]
    Test = as.data.frame(Test)
              Train$cat = as.numeric(as.factor(Train$cat))
          Test$cat = as.numeric(as.factor(Test$cat))
   dfTmp <- NULL 
   iResample = 1

glmTrain <- glm(as.factor(Train$cat)~.,data=Train[,2:103],family=binomial)
  glmTestPred <- predict(glmTrain, newdata=Test[,2:103], type="response") > 0.5
  tmpVals <- summPreds(as.numeric(glmTestPred)+1,as.factor(Test$cat))
  dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type="LR",metric=names(tmpVals),value=tmpVals))
  
  
        # KNN:
#     for ( kTmp in c(10,20,150,300) ) {
# 
#       knnTestPred <- knn(Train[,2:116],Test[,2:116],Train$cat,k=kTmp)
#       tmpVals <- summPreds(as.numeric(knnTestPred),as.numeric(Test$cat))
#       dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type=paste0("K",kTmp),metric=names(tmpVals),value=tmpVals))
# }

```



Here is the validation code set.  Right now I am only using a few models but ideally I would include more.

```{r testerr,warning=FALSE}
# warning=FALSE in knitr clause prevents well understood warnings from cluttering the output
dfTmp <- NULL
for ( iResample in 1:2 ) {
  for ( iSim in 1:100 ) {
    bTrain = sample(1:nrow(model_4_21_95wRAW),round(0.7*nrow(model_4_21_95wRAW)))
    Train <- model_4_21_95wRAW[bTrain,2:104]
    Test <- model_4_21_95wRAW[-bTrain,2:104]
    Test = as.data.frame(Test)
    Train = as.data.frame(Train)
    if ( iResample == 2 ) {
          bTrain = sample(1:nrow(model_4_21_95wRAW),round(0.7*nrow(model_4_21_95wRAW)),replace=TRUE)
    Train <- model_4_21_95wRAW[bTrain,2:104]
    Test <- model_4_21_95wRAW[-bTrain,2:104]
    Test = as.data.frame(Test)
    Train = as.data.frame(Train)
    }
    # logistic regression:
  glmTrain <- glm(as.factor(Train$cat)~.,data=Train[,2:103],family=binomial)
  glmTestPred <- predict(glmTrain, newdata=Test[,2:103], type="response") > 0.5
  tmpVals <- summPreds(as.numeric(glmTestPred)+1,as.factor(Test$cat))
  dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type="LR",metric=names(tmpVals),value=tmpVals))
#RF
Y = factor(as.numeric(as.factor(Train$cat)))
rfTmp <- randomForest(Y~.,data=Train[,2:103],mTry = 10)
rfTestPred <- predict(rfTmp,newdata=Test)
mseTest <- mean((as.numeric(factor(Test$cat))-as.numeric(rfTestPred))) #Calculating the mean squared error
tmpVals <- summPreds(as.numeric(factor(Test$cat)),as.numeric(rfTestPred))
dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type="RF",metric=names(tmpVals),value=tmpVals))
# LDA:
ldaTrain <- lda(Train$cat~.,data=Train[,2:103])
ldaTestPred <- predict(ldaTrain, newdata=Test[,2:103])
tmpVals <- summPreds(as.numeric(ldaTestPred$class),as.factor(Test$cat))
dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type="LDA",metric=names(tmpVals),value=tmpVals))
# NB:
Y = factor(as.numeric(as.factor(Train$cat)))
nbTrain <- naiveBayes(Y~.,data=Train[,2:103])
nbTestPred = predict(nbTrain, newdata=Test[,2:103])
tmpVals = summPreds(as.numeric(nbTestPred),as.numeric(as.factor(Test$cat)))
dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type="NB",metric=names(tmpVals),value=tmpVals))

  }
}
```


```{r}
p = ggplot(dfTmp,aes(x=type,y=100*value,colour = type)) + geom_boxplot(fill="white") + geom_point() + facet_wrap(~resample+metric,ncol=4,scales="free") + xlab("") + ylab("") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme_bw()

p
```
Quality Control
Here what I am doing is running Rnadom Forst using random outcomes. This will check to see if Random Forst can make predictions if the gene classifications are not real.

```{r}
scrambled_outcome = sample(Train$cat)
scrambled_outcome_test = sample(Test$cat)
length(scrambled_outcome)
length(Train$cat)
```


```{r}

Y = factor(as.numeric(as.factor(scrambled_outcome)))
Y_test = factor(as.numeric(as.factor(scrambled_outcome_test)))
rfTmp <- randomForest(Y~.,data=Train[,2:103],mTry = 10)
rfTestPred <- predict(rfTmp,newdata=Test)
mseTest_scrambled <- mean((as.numeric(factor(Y_test))-as.numeric(rfTestPred))) #Calculating the mean squared error
tmpVals_scrambled <- summPreds(as.numeric(factor(Y_test)),as.numeric(rfTestPred))


```

Running the same thing many times, in this case I am scrambling the outcome variables.

```{r}
Y_s = factor(as.numeric(as.factor(scrambled_outcome)))
Y_test = factor(as.numeric(as.factor(scrambled_outcome_test)))
Y = factor(as.numeric(as.factor(Train$cat)))

dfTmp <- NULL
for ( iResample in 1:2 ) {
  for ( iSim in 1:100 ) {
 
  
  scrambled_outcome_eachTime = sample(Train$cat)
  scrambled_outcome_test_eachTime = sample(Test$cat)  
  Y_s_et = factor(as.numeric(as.factor(scrambled_outcome_eachTime)))
  Y_test_et = factor(as.numeric(as.factor(scrambled_outcome_test_eachTime)))  
    
    
  rfTmp <- randomForest(Y~.,data=Train[,2:103],mTry = 10)
  rfTestPred <- predict(rfTmp,newdata=Test)
  mseTest <- mean((as.numeric(factor(Test$cat))-as.numeric(rfTestPred))) #Calculating the mean squared error
  tmpVals <- summPreds(as.numeric(factor(Test$cat)),as.numeric(rfTestPred))
  dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type="RF",metric=names(tmpVals),value=tmpVals))  
    
  rfTmpS <- randomForest(Y_s~.,data=Train[,2:103],mTry = 10)
  rfTestPredS <- predict(rfTmpS,newdata=Test)
  mseTest_scrambled <- mean((as.numeric(factor(Y_test))-as.numeric(rfTestPredS))) #Calculating the mean squared error
  tmpVals_scrambled <- summPreds(as.numeric(factor(Y_test)),as.numeric(rfTestPredS))
  dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type="RF Scrambled",metric=names(tmpVals_scrambled),value=tmpVals_scrambled))

  rfTmpET <- randomForest(Y_s_et~.,data=Train[,2:103],mTry = 10)
  rfTestPredET <- predict(rfTmpET,newdata=Test)
  mseTest_scrambledET <- mean((as.numeric(factor(Y_test_et))-as.numeric(rfTestPredET))) #Calculating the mean squared error
  tmpVals_scrambledET <- summPreds(as.numeric(factor(Y_test_et)),as.numeric(rfTestPredET))
  dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[iResample],type="RF Scrambled Each Time",metric=names(tmpVals_scrambledET),value=tmpVals_scrambledET))

  }
}
```


```{r}
p = ggplot(dfTmp,aes(x=type,y=100*value,colour = type)) + geom_boxplot(fill="white") + geom_point() + facet_wrap(~resample+metric,ncol=4,scales="free") + xlab("") + ylab("") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme_bw()

p
```



```{r include=FALSE}
   holder = matrix(0,100,2)
    cNames = paste("col_", letters[1:20], sep="")
    cNames = c(cNames,"cat")
    cNames = as.character(cNames)
dfTmp <- NULL
for ( iResample in 1:2 ) {
  for ( iSim in 1:100 ) {
    
    #Normal
    rfTmp_normal <- randomForest(Y~.,data=Train[,2:43],mTry = 10)
    rfTestPred_normal <- predict(rfTmp_normal,newdata=Test[,2:43])
    mseTest_normal <- mean((as.numeric(factor(Test$cat))-as.numeric(rfTestPred_normal))) #Calculating the mean squared error
    tmpVals_normal <- summPreds(as.numeric(factor(Test$cat)),as.numeric(rfTestPred_normal))
    dfTmp <- rbind(dfTmp,data.frame(resample=c("Bootstrap Round 1","Bootstrap Round 2")[iResample],type="RF normal",metric=names(tmpVals_normal),value=tmpVals_normal)) 
    
    #Random columns
    colsToUse = sample(2:43,20)
    rfTmp_rand <- randomForest(Y~.,data=Train[,colsToUse],mTry = 10)
    rfTestPred_rand <- predict(rfTmp_rand,newdata=Test[,colsToUse])
    mseTest_rand <- mean((as.numeric(factor(Test$cat))-as.numeric(rfTestPred_rand))) #Calculating the mean squared error
    tmpVals_rand <- summPreds(as.numeric(factor(Test$cat)),as.numeric(rfTestPred_rand))
    dfTmp <- rbind(dfTmp,data.frame(resample=c("Bootstrap Round 1","Bootstrap Round 2")[iResample],type="RF Random Columns",metric=names(tmpVals_rand),value=tmpVals_rand)) 
    
    #random in that the training and test sets use different columns
    rfTestPred = NULL
    mtry = sample(1:20,1)
    train = sample(Train[,2:43],20)
    train = cbind(train,Train$cat)
    colnames(train) = cNames
    test = sample(Test[,2:43],20)
    test = cbind(test,Test$cat)
    colnames(test) = cNames
    train$cat = as.factor(train$cat)
    rfTmp <- randomForest(train$cat ~.,data=train[,1:20],mTry = mtry,ntree = 500)
    rfTestPred <- predict(rfTmp,newdata = test)
   mseTest <- mean((as.numeric(factor(test$cat))-as.numeric(rfTestPred))) #Calculating the mean squared error
    v = as.numeric(test$cat)
    w = as.numeric(rfTestPred)
    holder[iSim,iResample] = mseTest
    tmpVals <- summPreds(as.numeric(rfTestPred),as.numeric(factor(test$cat)))
    dfTmp <- rbind(dfTmp,data.frame(resample=c("Bootstrap Round 1","Bootstrap Round 2")[iResample],type="RF random",metric=names(tmpVals),value=tmpVals)) 

  }
}
```


```{r}
p = ggplot(dfTmp,aes(x=type,y=100*value,colour = type)) + geom_boxplot(fill="white") + geom_point() + facet_wrap(~resample+metric,ncol=4,scales="free") + xlab("") + ylab("") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme_bw()

p
```

This is junk space for testing
```{r}
    rfTestPred = NULL
    cNames = paste("col_", letters[1:20], sep="")
    cNames = c(cNames,"cat")
    cNames = as.character(cNames)
    train = sample(Train[,2:102],20)
    train = cbind(train,Train$cat)
    colnames(train) = cNames
    test = sample(Test[,2:102],20)
    test = cbind(test,Test$cat)
    colnames(test) = cNames
    rfTmp <- randomForest(train$cat~.,data=train[,1:20],mTry = 10)
    rfTestPred <- predict(rfTmp,newdata = test)
    mseTest <- mean((as.numeric(factor(test$cat))-as.numeric(rfTestPred))) #Calculating the mean squared error
    v = as.numeric(test$cat)
    w = as.numeric(rfTestPred)
    tmpVals <- summPreds(as.numeric(rfTestPred),as.numeric(factor(test$cat)))
    dfTmp <- rbind(dfTmp,data.frame(resample=c("Validation","Bootstrap")[1],type="RF",metric=names(tmpVals),value=tmpVals))  

    
    
    
 # summPreds <- function(inpPred,inpTruth,inpMetrNms=c("err","acc","sens","spec")) {
    
  inpMetrNms=c("err","acc","sens","spec")  
  retVals <- numeric()
  for ( metrTmp in inpMetrNms ) {
    retVals[metrTmp] <- performance(prediction(rfTestPred,test$cat),measure=metrTmp)@y.values[[1]][2]
  }
  retVals
#}  
    
```

